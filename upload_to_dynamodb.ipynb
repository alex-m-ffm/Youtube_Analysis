{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading credentials from file...\n",
      "Refreshing Access Token...\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from io import FileIO\n",
    "from decimal import Decimal\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# this is using login to the API via OAuth which enables private data for your own channel\n",
    "\n",
    "credentials = None\n",
    "\n",
    "# token.pickle stores the user's credentials from previously successful logins\n",
    "if os.path.exists('token.pickle'):\n",
    "    print('Loading credentials from file...')\n",
    "    with open('token.pickle', 'rb') as token:\n",
    "        credentials = pickle.load(token)\n",
    "\n",
    "if not credentials or not credentials.valid:\n",
    "    if credentials and credentials.expired and credentials.refresh_token:\n",
    "        print('Refreshing Access Token...')\n",
    "        credentials.refresh(Request())\n",
    "    else:\n",
    "        print('Fetching new tokens...')\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(\n",
    "            'client_secrets.json',\n",
    "            scopes=[\n",
    "                'https://www.googleapis.com/auth/youtube.readonly',\n",
    "                'https://www.googleapis.com/auth/yt-analytics.readonly'\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "        flow.run_local_server(port=8080, prompt='consent')\n",
    "        # you receive access and refresh tokens\n",
    "        # refresh token is to get new acccess tokens\n",
    "        credentials = flow.credentials\n",
    "\n",
    "        #save\n",
    "        with open('token.pickle', 'wb') as f:\n",
    "            print('Saving credentials for future use...')\n",
    "            pickle.dump(credentials, f)\n",
    "\n",
    "# Create YouTube API object\n",
    "youtubeReporting = build('youtubereporting', 'v1', credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download report from YouTube Reporting API\n",
    "def download_report(youtubeReporting, report_url, local_file):\n",
    "    request = youtubeReporting.media().download(resourceName='')\n",
    "    request.uri = report_url\n",
    "    with FileIO(local_file, mode='wb') as fh:\n",
    "        downloader = MediaIoBaseDownload(fh, request, chunksize=-1)\n",
    "        done = False\n",
    "        while done is False:\n",
    "            status, done = downloader.next_chunk()\n",
    "    #         if status:\n",
    "    #             print('Download %d%%.' % int(status.progress() * 100))\n",
    "    # print('Download Complete!')\n",
    "\n",
    "# Function to convert date from YYYYMMDD format to ISO 8601 format\n",
    "def convert_date(date_series):\n",
    "    return pd.to_datetime(date_series, format='%Y%m%d').dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# Function to upload data to DynamoDB\n",
    "def upload_to_basic_table(df):\n",
    "    table = dynamodb.Table('channel_basic_a2')\n",
    "    with table.batch_writer() as batch:\n",
    "        for index, row in df.iterrows():\n",
    "            item = {\n",
    "                'composite_key': row['composite_key'],\n",
    "                'createTime': row['createTime'],\n",
    "                'date': str(row['date']),\n",
    "                'channel_id': row['channel_id'],\n",
    "                'video_id': row['video_id'],\n",
    "                'live_or_on_demand': row['live_or_on_demand'],\n",
    "                'subscribed_status': row['subscribed_status'],\n",
    "                'country_code': row['country_code'],\n",
    "                'views': row['views'],\n",
    "                'comments': row['comments'],\n",
    "                'likes': row['likes'],\n",
    "                'dislikes': row['dislikes'],\n",
    "                'shares': row['shares'],\n",
    "                'watch_time_minutes': Decimal(row['watch_time_minutes']).quantize(Decimal('0.01')),\n",
    "                'average_view_duration_seconds': Decimal(row['average_view_duration_seconds']).quantize(Decimal('0.01')),\n",
    "                'average_view_duration_percentage': Decimal(row['average_view_duration_percentage']).quantize(Decimal('0.01')),\n",
    "                'annotation_impressions': row['annotation_impressions'],\n",
    "                'annotation_clickable_impressions': row['annotation_clickable_impressions'],\n",
    "                'annotation_clicks': row['annotation_clicks'],\n",
    "                'annotation_click_through_rate': row['annotation_click_through_rate'],\n",
    "                'annotation_closable_impressions': row['annotation_closable_impressions'],\n",
    "                'annotation_closes': row['annotation_closes'],\n",
    "                'annotation_close_rate': row['annotation_close_rate'],\n",
    "                'card_teaser_impressions': row['card_teaser_impressions'],\n",
    "                'card_teaser_clicks': row['card_teaser_clicks'],\n",
    "                'card_teaser_click_rate': row['card_teaser_click_rate'],\n",
    "                'card_impressions': row['card_impressions'],\n",
    "                'card_clicks': row['card_clicks'],\n",
    "                'card_click_rate': row['card_click_rate'],\n",
    "                'subscribers_gained': row['subscribers_gained'],\n",
    "                'subscribers_lost': row['subscribers_lost'],\n",
    "                'videos_added_to_playlists': row['videos_added_to_playlists'],\n",
    "                'videos_removed_from_playlists': row['videos_removed_from_playlists'],\n",
    "                'red_views': row['red_views'],\n",
    "                'red_watch_time_minutes': Decimal(row['red_watch_time_minutes']).quantize(Decimal('0.01'))\n",
    "            }\n",
    "            batch.put_item(Item=item)\n",
    "\n",
    "def upload_to_combined_table(df):\n",
    "    table = dynamodb.Table('channel_combined_a2')\n",
    "    with table.batch_writer() as batch:\n",
    "        for index, row in df.iterrows():\n",
    "            item = {\n",
    "                'composite_key': row['composite_key'],\n",
    "                'createTime': row['createTime'],\n",
    "                'date': str(row['date']),\n",
    "                'channel_id': row['channel_id'],\n",
    "                'video_id': row['video_id'],\n",
    "                'live_or_on_demand': row['live_or_on_demand'],\n",
    "                'subscribed_status': row['subscribed_status'],\n",
    "                'country_code': row['country_code'],\n",
    "                'playback_location_type': row['playback_location_type'],\n",
    "                'traffic_source_type': row['traffic_source_type'], \n",
    "                'device_type': row['traffic_source_type'], \n",
    "                'operating_system': row['traffic_source_type'], \n",
    "                'views': row['traffic_source_type'],\n",
    "                'watch_time_minutes': Decimal(row['traffic_source_type']), \n",
    "                'average_view_duration_seconds': Decimal(row['traffic_source_type']).quantize(Decimal('0.01')),\n",
    "                'average_view_duration_percentage': Decimal(row['traffic_source_type']).quantize(Decimal('0.01')), \n",
    "                'red_views': row['traffic_source_type'],\n",
    "                'red_watch_time_minutes': Decimal(row['traffic_source_type']).quantize(Decimal('0.01'))\n",
    "            }\n",
    "            batch.put_item(Item=item)\n",
    "\n",
    "# Function to upload data to DynamoDB\n",
    "def upload_to_demo_table(df):\n",
    "    table = dynamodb.Table('channel_demographics_a1')\n",
    "    with table.batch_writer() as batch:\n",
    "        for index, row in df.iterrows():\n",
    "            item = {\n",
    "                'composite_key': row['composite_key'],\n",
    "                'createTime': row['createTime'],\n",
    "                'date': str(row['date']),\n",
    "                'channel_id': row['channel_id'],\n",
    "                'video_id': row['video_id'],\n",
    "                'live_or_on_demand': row['live_or_on_demand'],\n",
    "                'subscribed_status': row['subscribed_status'],\n",
    "                'country_code': row['country_code'],\n",
    "                'age_group': row['age_group'],\n",
    "                'gender': row['gender'],\n",
    "                'views_percentage': Decimal(row['views_percentage']).quantize(Decimal('0.01'))\n",
    "            }\n",
    "            batch.put_item(Item=item)\n",
    "\n",
    "# Function to upload data to DynamoDB\n",
    "def upload_to_sharing_table(df):\n",
    "    table = dynamodb.Table('channel_sharing_service_a1')\n",
    "    with table.batch_writer() as batch:\n",
    "        for index, row in df.iterrows():\n",
    "            item = {\n",
    "                'composite_key': row['composite_key'],\n",
    "                'createTime': row['createTime'],\n",
    "                'date': str(row['date']),\n",
    "                'channel_id': row['channel_id'],\n",
    "                'video_id': row['video_id'],\n",
    "                'live_or_on_demand': row['live_or_on_demand'],\n",
    "                'subscribed_status': row['subscribed_status'],\n",
    "                'country_code': row['country_code'],\n",
    "                'sharing_service': row['sharing_service'],\n",
    "                'shares': row['shares'],\n",
    "            }\n",
    "            batch.put_item(Item=item)\n",
    "            \n",
    "\n",
    "# upload a dictionary from the jobs.reports().list() query to the reports table\n",
    "def upload_to_jobs_table(report):\n",
    "   table = dynamodb.Table('reports')\n",
    "   with table.batch_writer() as batch:\n",
    "      batch.put_item(Item=report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing keys: []\n"
     ]
    }
   ],
   "source": [
    "# Retrieve reports from YouTube Reporting API - basic report\n",
    "reports_result = youtubeReporting.jobs().reports().list(jobId='a7f41b3e-2b81-488d-8ed0-1f8c236e1a54').execute()\n",
    "\n",
    "table_name = 'channel_basic_a2'\n",
    "\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "\n",
    "available_report_ids = [report['id'] for report in reports_result['reports']]\n",
    "\n",
    "# check the reports table in DynamoDB whether the report has already been uploaded\n",
    "# Define the keys you want to retrieve\n",
    "ids_to_retrieve = [{'id': key} for key in available_report_ids]\n",
    "\n",
    "# Perform batch get item operation\n",
    "response = dynamodb.batch_get_item(\n",
    "    RequestItems={\n",
    "        'reports': {\n",
    "            'Keys': ids_to_retrieve\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Check if any items are returned\n",
    "if 'Responses' in response:\n",
    "    items = response['Responses']['reports']\n",
    "    existing_keys = [item['id'] for item in items]\n",
    "else:\n",
    "    print(\"No items returned for the specified keys.\")\n",
    "    existing_keys = []\n",
    "\n",
    "# create set difference between available reports and those already processed\n",
    "new_reports = set(available_report_ids) - set(existing_keys)\n",
    "\n",
    "# Create a new column containing the composite key\n",
    "composite_key_cols = ['date', 'channel_id', 'video_id', 'live_or_on_demand', 'subscribed_status', 'country_code']\n",
    "\n",
    "# Iterate through each report and process\n",
    "for report in reports_result['reports']:\n",
    "    if report['id'] in new_reports:\n",
    "        local_file = f\"reports/{report['id']}.csv\"\n",
    "        download_report(youtubeReporting, report['downloadUrl'], local_file)\n",
    "        with open(local_file, 'r') as file:\n",
    "            df = pd.read_csv(local_file)\n",
    "            if not df.empty:\n",
    "                df['createTime'] = report['createTime']\n",
    "                df['composite_key'] = df[composite_key_cols].astype(str).agg('_'.join, axis=1)\n",
    "                df['date'] = convert_date(df['date'])\n",
    "                upload_to_basic_table(df)\n",
    "                upload_to_jobs_table(report)\n",
    "        # Delete the file\n",
    "        os.remove(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing keys: []\n"
     ]
    }
   ],
   "source": [
    "# Retrieve reports from YouTube Reporting API - channel demographics\n",
    "reports_result = youtubeReporting.jobs().reports().list(jobId='4a7e6f19-e49f-4418-9800-f0ba979a8437').execute()\n",
    "\n",
    "table_name = 'channel_demographics_a1'\n",
    "\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "\n",
    "available_report_ids = [report['id'] for report in reports_result['reports']]\n",
    "\n",
    "# check the reports table in DynamoDB whether the report has already been uploaded\n",
    "# Define the keys you want to retrieve\n",
    "ids_to_retrieve = [{'id': key} for key in available_report_ids]\n",
    "\n",
    "# Perform batch get item operation\n",
    "response = dynamodb.batch_get_item(\n",
    "    RequestItems={\n",
    "        'reports': {\n",
    "            'Keys': ids_to_retrieve\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Check if any items are returned\n",
    "if 'Responses' in response:\n",
    "    items = response['Responses']['reports']\n",
    "    existing_keys = [item['id'] for item in items]\n",
    "else:\n",
    "    print(\"No items returned for the specified keys.\")\n",
    "    existing_keys = []\n",
    "\n",
    "# create set difference between available reports and those already processed\n",
    "new_reports = set(available_report_ids) - set(existing_keys)\n",
    "\n",
    "# Create a new column containing the composite key\n",
    "composite_key_cols = ['date', 'channel_id', 'video_id', 'live_or_on_demand', 'subscribed_status', 'country_code', 'age_group', 'gender']\n",
    "\n",
    "# Iterate through each report and process\n",
    "for report in reports_result['reports']:\n",
    "    if report['id'] in new_reports:\n",
    "        local_file = f\"reports/{report['id']}.csv\"\n",
    "        download_report(youtubeReporting, report['downloadUrl'], local_file)\n",
    "        with open(local_file, 'r') as file:\n",
    "            df = pd.read_csv(file)\n",
    "            if not df.empty:\n",
    "                df['createTime'] = report['createTime']\n",
    "                df['composite_key'] = df[composite_key_cols].astype(str).agg('_'.join, axis=1)\n",
    "                df['date'] = convert_date(df['date'])\n",
    "                upload_to_demo_table(df)\n",
    "                upload_to_jobs_table(report)\n",
    "        # Delete the file\n",
    "        os.remove(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing keys: []\n"
     ]
    }
   ],
   "source": [
    "# Retrieve reports from YouTube Reporting API - channel sharing\n",
    "reports_result = youtubeReporting.jobs().reports().list(jobId='bff80780-0f0f-4caa-af3e-de968ec64e9e').execute()\n",
    "\n",
    "table_name = 'channel_sharing_service_a1'\n",
    "\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "\n",
    "available_report_ids = [report['id'] for report in reports_result['reports']]\n",
    "\n",
    "# check the reports table in DynamoDB whether the report has already been uploaded\n",
    "# Define the keys you want to retrieve\n",
    "ids_to_retrieve = [{'id': key} for key in available_report_ids]\n",
    "\n",
    "# Perform batch get item operation\n",
    "response = dynamodb.batch_get_item(\n",
    "    RequestItems={\n",
    "        'reports': {\n",
    "            'Keys': ids_to_retrieve\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Check if any items are returned\n",
    "if 'Responses' in response:\n",
    "    items = response['Responses']['reports']\n",
    "    existing_keys = [item['id'] for item in items]\n",
    "else:\n",
    "    print(\"No items returned for the specified keys.\")\n",
    "    existing_keys = []\n",
    "\n",
    "# create set difference between available reports and those already processed\n",
    "new_reports = set(available_report_ids) - set(existing_keys)\n",
    "\n",
    "# Create a new column containing the composite key\n",
    "composite_key_cols = ['date', 'channel_id', 'video_id', 'live_or_on_demand', 'subscribed_status', 'country_code', 'sharing_service']\n",
    "\n",
    "# Iterate through each report and process\n",
    "for report in reports_result['reports']:\n",
    "    if report['id'] in new_reports:\n",
    "        local_file = f\"reports/{report['id']}.csv\"\n",
    "        download_report(youtubeReporting, report['downloadUrl'], local_file)\n",
    "        with open(local_file, 'r') as file:\n",
    "            df = pd.read_csv(file)\n",
    "            if not df.empty:\n",
    "                df['createTime'] = report['createTime']\n",
    "                df['composite_key'] = df[composite_key_cols].astype(str).agg('_'.join, axis=1)\n",
    "                df['date'] = convert_date(df['date'])\n",
    "                upload_to_sharing_table(df)\n",
    "                upload_to_jobs_table(report)\n",
    "        # Delete the file\n",
    "        os.remove(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing keys: []\n"
     ]
    }
   ],
   "source": [
    "# Retrieve reports from YouTube Reporting API - combined table\n",
    "reports_result = youtubeReporting.jobs().reports().list(jobId='82bc9b78-afbf-470e-8c1f-e9a7d2fe280d').execute()\n",
    "\n",
    "table_name = 'channel_combined_a2'\n",
    "\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "\n",
    "available_report_ids = [report['id'] for report in reports_result['reports']]\n",
    "\n",
    "# check the reports table in DynamoDB whether the report has already been uploaded\n",
    "# Define the keys you want to retrieve\n",
    "ids_to_retrieve = [{'id': key} for key in available_report_ids]\n",
    "\n",
    "# Perform batch get item operation\n",
    "response = dynamodb.batch_get_item(\n",
    "    RequestItems={\n",
    "        'reports': {\n",
    "            'Keys': ids_to_retrieve\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Check if any items are returned\n",
    "if 'Responses' in response:\n",
    "    items = response['Responses']['reports']\n",
    "    existing_keys = [item['id'] for item in items]\n",
    "else:\n",
    "    print(\"No items returned for the specified keys.\")\n",
    "    existing_keys = []\n",
    "\n",
    "# create set difference between available reports and those already processed\n",
    "new_reports = set(available_report_ids) - set(existing_keys)\n",
    "\n",
    "# Create a new column containing the composite key\n",
    "composite_key_cols = ['date', 'channel_id', 'video_id', 'live_or_on_demand', 'subscribed_status', 'country_code', \n",
    "                       'playback_location_type', 'traffic_source_type', 'device_type', 'operating_system']\n",
    "\n",
    "# Iterate through each report and process\n",
    "for report in reports_result['reports']:\n",
    "    if report['id'] in new_reports:\n",
    "        local_file = f\"reports/{report['id']}.csv\"\n",
    "        download_report(youtubeReporting, report['downloadUrl'], local_file)\n",
    "        with open(local_file, 'r') as file:\n",
    "            df = pd.read_csv(file)\n",
    "            if not df.empty:\n",
    "                df['createTime'] = report['createTime']\n",
    "                df['composite_key'] = df[composite_key_cols].astype(str).agg('_'.join, axis=1)\n",
    "                df['date'] = convert_date(df['date'])\n",
    "                upload_to_combined_table(df)\n",
    "                upload_to_jobs_table(report)\n",
    "        # Delete the file\n",
    "        os.remove(local_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
